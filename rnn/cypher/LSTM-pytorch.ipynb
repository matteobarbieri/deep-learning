{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUVEMVEMNMERPDRF\n"
     ]
    }
   ],
   "source": [
    "key = 13\n",
    "vocab = [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ-']\n",
    "\n",
    "\n",
    "def encrypt(text):\n",
    "    \"\"\"Returns the encrypted form of 'text'.\"\"\"\n",
    "    indexes = [vocab.index(char) for char in text]\n",
    "    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\n",
    "    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\n",
    "    encrypted = ''.join(encrypted_chars)\n",
    "    return encrypted\n",
    "\n",
    "print(encrypt('THIS-IS-A-SECRET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "message_length = 32\n",
    "\n",
    "def dataset(num_examples):\n",
    "    \n",
    "    \"\"\"Returns a list of 'num_examples' pairs of the form (encrypted, original).\n",
    "\n",
    "    Both elements of the pair are tensors containing indexes of each character\n",
    "    of the corresponding encrypted or original message.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for x in range(num_examples):\n",
    "        ex_out = ''.join([random.choice(vocab) for x in range(message_length)])\n",
    "        # may be: MANR-TQNNAFEGIDE-OXQZANSVEMJXWSU\n",
    "        ex_in = encrypt(''.join(ex_out))\n",
    "        # may be: ZN-DMFC--NSRTVQRMAJCLN-EHRZWJIEG\n",
    "        ex_in = [vocab.index(x) for x in ex_in]\n",
    "        # may be: [25, 13, 26, 3, 12, 5, 2, 26, 26, ...\n",
    "        ex_out = [vocab.index(x) for x in ex_out]\n",
    "        # may be: [12, 0, 13, 17, 26, 19, 16, 13, ...\n",
    "        dataset.append([torch.tensor(ex_in), torch.tensor(ex_out)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 5\n",
    "\n",
    "# Step 1\n",
    "embed = torch.nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10\n",
    "\n",
    "# Step 1\n",
    "lstm = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_hidden(hidden_dim):\n",
    "    return (torch.zeros(1, 32, hidden_dim),\n",
    "            torch.zeros(1, 32, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an affine transformation to go return the right size\n",
    "linear = torch.nn.Linear(hidden_dim, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) + \n",
    "                             list(lstm.parameters()) +\n",
    "                             list(linear.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 3.0144\n",
      "Epoch: 1\n",
      "Loss: 2.2816\n",
      "Epoch: 2\n",
      "Loss: 1.4830\n",
      "Epoch: 3\n",
      "Loss: 0.8648\n",
      "Epoch: 4\n",
      "Loss: 0.5933\n",
      "Epoch: 5\n",
      "Loss: 0.3174\n",
      "Epoch: 6\n",
      "Loss: 0.2276\n",
      "Epoch: 7\n",
      "Loss: 0.1489\n",
      "Epoch: 8\n",
      "Loss: 0.0890\n",
      "Epoch: 9\n",
      "Loss: 0.0442\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_examples = 128\n",
    "\n",
    "accuracies, max_accuracy = [], 0\n",
    "\n",
    "for x in range(num_epochs):\n",
    "    print('Epoch: {}'.format(x))\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "        # encrypted.size() = [64]\n",
    "        lstm_in = embed(encrypted)\n",
    "        # lstm_in.size() = [64, 5]. This is a 2D tensor, but LSTM expects \n",
    "        # a 3D tensor. So we insert a fake dimension.\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        \n",
    "        # lstm_in.size() = [64, 1, 5]\n",
    "        # Get outputs from the LSTM.\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden(hidden_dim))\n",
    "        # lstm_out.size() = [64, 1, 10]\n",
    "        # Apply the affine transform.\n",
    "        scores = linear(lstm_out)\n",
    "        # scores.size() = [64, 1, 27], but loss_fn expects a tensor\n",
    "        # of size [64, 27, 1]. So we switch the second and third dimensions.\n",
    "        scores = scores.transpose(1, 2)\n",
    "        # original.size() = [64], but original should also be a 2D tensor\n",
    "        # of size [64, 1]. So we insert a fake dimension.\n",
    "        original = original.unsqueeze(1)\n",
    "        # Calculate loss.\n",
    "        loss = loss_fn(scores, original)\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Loss: {:6.4f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for encrypted, original in dataset(num_examples):\n",
    "            lstm_in = embed(encrypted)\n",
    "            lstm_in = lstm_in.unsqueeze(1)\n",
    "            lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden(hidden_dim))\n",
    "            scores = linear(lstm_out)\n",
    "            # Compute a softmax over the outputs\n",
    "            predictions = torch.softmax(scores, dim=2)\n",
    "            # Choose the letter with the maximum probability\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            # Remove fake dimension\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            # Calculate accuracy\n",
    "            matches += torch.eq(batch_out, original).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
